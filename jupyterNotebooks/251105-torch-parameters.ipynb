{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49efd251-d418-43b2-9cd6-cb37cc1bc665",
   "metadata": {},
   "source": [
    "- eton@251105\n",
    "### nn.Parameter是PyTorch中用于创建可学习参数的重要工具，主要用于：\n",
    "\n",
    "- 将张量标记为模型参数，使其能被自动更新\n",
    "- 在自定义层和模型中创建可训练的权重和偏置\n",
    "- 实现参数共享和自定义初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb96a80d-698e-40aa-b1ef-375b3ee4a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "411aa66d-ca8b-4822-b85e-c76e8222ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 创建一个简单的模型\n",
    "class ModelWithParameters(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithParameters, self).__init__()\n",
    "        # 定义可学习参数\n",
    "        self.weight = nn.Parameter(torch.randn(3, 2))\n",
    "        self.bias = nn.Parameter(torch.zeros(3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (10,2) multiply (2,3) which is transpose of weight. result=(10,3)\n",
    "        print(\"bias=\", self.bias)\n",
    "        return torch.matmul(x, self.weight.t()) + self.bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e319646c-cfb4-474b-8838-a6a9a55ddd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[-0.6055, -1.3876],\n",
      "        [ 1.1386,  1.3463],\n",
      "        [-0.9156,  0.3331],\n",
      "        [-1.0399,  1.0583],\n",
      "        [-0.6382,  1.1240],\n",
      "        [-0.2581,  0.8191],\n",
      "        [-1.2863,  0.2016],\n",
      "        [ 0.6478, -0.5764],\n",
      "        [ 0.0757,  1.0395],\n",
      "        [-0.7400,  0.6699]])\n",
      "y: tensor([0, 0, 2, 0, 2, 1, 1, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 创建模型、数据、损失函数和优化器\n",
    "model = ModelWithParameters()\n",
    "x = torch.randn(10, 2)  # 10个样本，每个样本2个特征\n",
    "y = torch.randint(0, 3, (10,))  # 10个标签，3个类别\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"x:\",x)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a210ad8-b176-41f2-b0b5-6cc26733b170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias= Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n",
      "Epoch 1, Loss: 1.2970\n",
      "Weight mean: 0.2148\n",
      "Bias mean: 0.0000\n",
      "bias= Parameter containing:\n",
      "tensor([-0.0016, -0.0010,  0.0026], requires_grad=True)\n",
      "Epoch 2, Loss: 1.2946\n",
      "Weight mean: 0.2148\n",
      "Bias mean: -0.0000\n",
      "bias= Parameter containing:\n",
      "tensor([-0.0032, -0.0020,  0.0051], requires_grad=True)\n",
      "Epoch 3, Loss: 1.2923\n",
      "Weight mean: 0.2148\n",
      "Bias mean: -0.0000\n",
      "bias= Parameter containing:\n",
      "tensor([-0.0047, -0.0030,  0.0077], requires_grad=True)\n",
      "Epoch 4, Loss: 1.2899\n",
      "Weight mean: 0.2148\n",
      "Bias mean: -0.0000\n",
      "bias= Parameter containing:\n",
      "tensor([-0.0063, -0.0039,  0.0102], requires_grad=True)\n",
      "Epoch 5, Loss: 1.2876\n",
      "Weight mean: 0.2148\n",
      "Bias mean: -0.0000\n"
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "for epoch in range(5):\n",
    "    # 前向传播\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # 反向传播和优化\n",
    "    optimizer.zero_grad()  # 清除梯度\n",
    "    loss.backward()        # 反向传播计算梯度\n",
    "    optimizer.step()       # 更新参数\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "    # 打印参数值变化\n",
    "    print(f'Weight mean: {model.weight.mean().item():.4f}')\n",
    "    print(f'Bias mean: {model.bias.mean().item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0731c7-98f3-49af-80c1-b652a36e9bdd",
   "metadata": {},
   "source": [
    "### - PyTorch中optim.LBFGS优化器的使用详解\n",
    "optim.LBFGS是PyTorch中实现的L-BFGS（Limited-memory Broyden-Fletcher-Goldfarb-Shanno）算法的优化器。它是一种拟牛顿法，适用于需要精确优化的小批量或全批量训练场景。\n",
    "\n",
    "### - LBFGS优化器的特点\n",
    "与常见的SGD或Adam优化器相比，LBFGS具有以下特点：\n",
    "\n",
    "- 使用二阶导数信息（通过有限内存近似），收敛速度通常更快\n",
    "- 需要计算完整的损失函数（通常使用全批量）\n",
    "- 需要定义一个闭包函数来重新计算损失和梯度\n",
    "- 对于小数据集和需要精确优化的场景效果很好（如您代码中的温度参数校准）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a58cd39e-9877-40ef-a2cf-9eda5af2ed9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, x: 1.0000, loss: 1.0000\n",
      "Step: 0, x: 1.1000, loss: 0.8100\n",
      "Step: 0, x: 1.1900, loss: 0.6561\n",
      "Step: 0, x: 1.2710, loss: 0.5314\n",
      "Step: 0, x: 1.3439, loss: 0.4305\n",
      "Step: 0, x: 1.4095, loss: 0.3487\n",
      "Step: 0, x: 1.4686, loss: 0.2824\n",
      "Step: 0, x: 1.5217, loss: 0.2288\n",
      "Step: 0, x: 1.5695, loss: 0.1853\n",
      "Step: 0, x: 1.6126, loss: 0.1501\n",
      "Step: 0, x: 1.6513, loss: 0.1216\n",
      "Step: 0, x: 1.6862, loss: 0.0985\n",
      "Step: 0, x: 1.7176, loss: 0.0798\n",
      "Step: 0, x: 1.7458, loss: 0.0646\n",
      "Step: 0, x: 1.7712, loss: 0.0523\n",
      "Step: 0, x: 1.7941, loss: 0.0424\n",
      "Step: 0, x: 1.8147, loss: 0.0343\n",
      "Step: 0, x: 1.8332, loss: 0.0278\n",
      "Step: 0, x: 1.8499, loss: 0.0225\n",
      "Step: 0, x: 1.8649, loss: 0.0182\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# 创建一个可学习的参数\n",
    "params = torch.nn.Parameter(torch.tensor([1.0]))\n",
    "\n",
    "# 创建LBFGS优化器，传入参数和配置\n",
    "optimizer = optim.LBFGS([params], lr=0.1, max_iter=20)\n",
    "\n",
    "# 定义目标函数（这里是f(x) = (x-2)^2）\n",
    "def criterion(x):\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "# 优化循环\n",
    "for i in range(1):\n",
    "    # LBFGS需要一个闭包函数来重新计算损失和梯度\n",
    "    def closure():\n",
    "        # 清除之前的梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 计算损失\n",
    "        loss = criterion(params)\n",
    "        # 反向传播计算梯度\n",
    "        loss.backward()\n",
    "        # 打印当前状态\n",
    "        print(f'Step: {i}, x: {params.item():.4f}, loss: {loss.item():.4f}')\n",
    "        # 返回损失值\n",
    "        return loss.item()\n",
    "    \n",
    "    # 执行优化步骤，传入闭包函数\n",
    "    optimizer.step(closure)\n",
    "warning_in_closure=\"\"\"\n",
    "torch/optim/lbfgs.py:457: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
    "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
    "  loss = float(closure())\n",
    "这个警告发生在：\n",
    "  1. LBFGS优化器内部尝试将闭包函数返回的损失值转换为Python浮点数\n",
    "  2. 当返回的损失张量仍然具有梯度跟踪（requires_grad=True）时\n",
    "  3. 直接将这样的张量转换为浮点数可能会导致计算图出现问题\n",
    "\n",
    "solution:\n",
    "- item() - 将单元素张量转换为Python标量\n",
    "- detach() - 从计算图中分离张量，停止梯度跟踪\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd68ec-0374-45d2-9894-c598faef18a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
