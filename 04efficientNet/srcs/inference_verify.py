#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹éªŒè¯é›†æ¨ç†è„šæœ¬ - è¯„ä¼°è®­ç»ƒå¥½çš„V75æ¨¡å‹
"""

import os
import sys
import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from PIL import Image
from tqdm import tqdm
from sklearn.metrics import roc_auc_score, confusion_matrix
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']  # æ”¯æŒä¸­æ–‡
plt.rcParams['axes.unicode_minus'] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

# =============================================================================
# é…ç½®
# =============================================================================
CONFIG = {
    'model_path': 'models/nodule_feature_cnn_v75/nodule_feature_cnn_v75_best_auc.pth',
    'verify_data': 'data/dataset_sop7/all_verify_sop.csv',
    'verify_root': '/Users/Shared/tars/nodule_images/',  # ä½¿ç”¨é¢„å¤„ç†åçš„ç»“èŠ‚å›¾åƒ
    'feature_mapping_file': 'core/utils/all_features_mapping_numer_v4.json',
    'batch_size': 32,
    'num_workers': 0,
    'dropout_rate': 0.4,
    'output_report': 'models/nodule_feature_cnn_v75/verification_report_preprocessed.txt',
    'output_csv': 'data/dataset_sop7/all_verify_sop_with_predictions.csv',  # å¸¦é¢„æµ‹ç»“æœçš„CSV
    'bom_threshold': 0.5,
}

# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================

def get_device():
    """è·å–æœ€ä½³å¯ç”¨è®¾å¤‡"""
    if torch.backends.mps.is_available(): return torch.device("mps")
    if torch.cuda.is_available(): return torch.device("cuda")
    return torch.device("cpu")

def extract_date_from_access_no(access_no):
    """ä»access_noä¸­æå–å¹´æœˆä¿¡æ¯"""
    try:
        parts = access_no.split('.')
        if len(parts) >= 2:
            datetime_str = parts[1]
            if len(datetime_str) >= 6:
                year_month = datetime_str[:6]
                if len(year_month) == 6 and year_month.isdigit():
                    return year_month
    except Exception:
        pass
    return None

# =============================================================================
# æ•°æ®é›†é¢„å¤„ç†
# =============================================================================

def filter_missing_images(df, image_root):
    """
    è¿‡æ»¤æ‰å›¾ç‰‡ä¸å­˜åœ¨çš„æ ·æœ¬
    
    Args:
        df: åŸå§‹DataFrame
        image_root: å›¾åƒæ ¹ç›®å½•
        
    Returns:
        è¿‡æ»¤åçš„DataFrameå’Œç¼ºå¤±æ ·æœ¬ä¿¡æ¯
    """
    missing_samples = []
    valid_rows = []
    
    print(f"ğŸ” æ£€æŸ¥å›¾åƒå®Œæ•´æ€§...")
    for idx, row in df.iterrows():
        access_no = row['access_no']
        sop_uid = row['sop_uid']
        image_path = os.path.join(image_root, str(access_no), f"{sop_uid}.jpg")
        
        if os.path.exists(image_path):
            valid_rows.append(row)
        else:
            missing_samples.append({
                'index': idx,
                'access_no': str(access_no),
                'sop_uid': str(sop_uid),
                'path': image_path
            })
    
    # ä»æœ‰æ•ˆè¡Œé‡æ–°æ„å»ºDataFrameï¼Œç¡®ä¿ç´¢å¼•ä»0å¼€å§‹
    df_filtered = pd.DataFrame(valid_rows).reset_index(drop=True)
    
    # æŠ¥å‘Šç»“æœ
    total_samples = len(df)
    valid_samples = len(df_filtered)
    missing_count = len(missing_samples)
    
    print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"  æœ‰æ•ˆæ ·æœ¬: {valid_samples} ({valid_samples/total_samples*100:.2f}%)")
    print(f"  ç¼ºå¤±æ ·æœ¬: {missing_count} ({missing_count/total_samples*100:.2f}%)")
    
    if missing_count > 0:
        print(f"  âš ï¸ è­¦å‘Š: {missing_count} ä¸ªæ ·æœ¬çš„å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†è¢«è·³è¿‡")
        if missing_count <= 10:
            print(f"  ç¼ºå¤±æ ·æœ¬åˆ—è¡¨:")
            for sample in missing_samples:
                print(f"    - {sample['access_no']}/{sample['sop_uid']}")
        else:
            print(f"  å‰10ä¸ªç¼ºå¤±æ ·æœ¬:")
            for sample in missing_samples[:10]:
                print(f"    - {sample['access_no']}/{sample['sop_uid']}")
            print(f"    ... è¿˜æœ‰ {missing_count - 10} ä¸ªç¼ºå¤±æ ·æœ¬")
    
    return df_filtered, missing_samples

# =============================================================================
# æ•°æ®é›†ç±»
# =============================================================================

class NoduleFeatureDataset(Dataset):
    """éªŒè¯æ•°æ®é›†ç±»"""
    def __init__(self, df, image_root, feature_mapping, transform=None):
        self.df = df.copy()
        self.image_root = image_root
        self.feature_mapping = feature_mapping
        self.transform = transform
        self.tasks = list(feature_mapping.keys())
        self.label_to_int_maps = self._create_label_maps()

    def _create_label_maps(self):
        label_maps = {}
        for task in self.tasks:
            label_maps[task] = self.feature_mapping[task]
        return label_maps

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image_path = os.path.join(self.image_root, str(row['access_no']), f"{row['sop_uid']}.jpg")

        try:
            image = Image.open(image_path).convert('RGB')
        except FileNotFoundError:
            # è¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºæˆ‘ä»¬åœ¨æ„é€ æ—¶å·²ç»è¿‡æ»¤äº†
            raise FileNotFoundError(
                f"Image not found: {image_path}\n"
                f"  access_no: {row['access_no']}\n"
                f"  sop_uid: {row['sop_uid']}"
            )

        if self.transform:
            image = self.transform(image)

        item = {
            'image': image,
            'access_no': row['access_no'],
            'sop_uid': row['sop_uid'],
            'type': row.get('type', 'unknown'),
            'idx': idx
        }

        # ä¸ºæ¯ä¸ªä»»åŠ¡è·å–æ ‡ç­¾
        for task in self.tasks:
            raw_val = row.get(task, np.nan)
            label_val = -1
            is_valid = 0.0

            if pd.notna(raw_val):
                key = str(raw_val) if not isinstance(raw_val, str) else raw_val
                if isinstance(raw_val, float) and raw_val.is_integer():
                    key = str(int(raw_val))

                task_map = self.label_to_int_maps[task]
                if key in task_map:
                    label_val = task_map[key]
                    is_valid = 1.0
                    if task == 'ti_rads':
                        label_val -= 1

            item[task] = torch.tensor(label_val, dtype=torch.long)
            item[f'{task}_valid'] = torch.tensor(is_valid, dtype=torch.float32)

        return item

# =============================================================================
# æ¨¡å‹å®šä¹‰
# =============================================================================

class MultiTaskNoduleCNN(nn.Module):
    """EfficientNet-B0å¤šä»»åŠ¡æ¨¡å‹"""
    def __init__(self, feature_mappings, dropout_rate=0.4):
        super().__init__()
        self.mappings = feature_mappings

        from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights
        self.backbone = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)
        backbone_features = 1280
        self.backbone.classifier = nn.Identity()

        self.shared_features = nn.Sequential(
            nn.Linear(backbone_features, 512),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.75)
        )

        self.heads = nn.ModuleDict()
        for task, mapping in self.mappings.items():
            num_classes = len(mapping)
            self.heads[task] = nn.Sequential(
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, num_classes)
            )

    def forward(self, x):
        features = self.backbone(x)
        shared = self.shared_features(features)
        outputs = {}
        for task, head in self.heads.items():
            outputs[task] = head(shared)
        return outputs

# =============================================================================
# æ¨ç†å’ŒæŒ‡æ ‡è®¡ç®—
# =============================================================================

def inference_and_collect(model, loader, device, threshold=0.5):
    """
    æ¨ç†å¹¶æ”¶é›†æ‰€æœ‰ç»“æœ
    
    Args:
        model: æ¨¡å‹
        loader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        threshold: è‰¯æ¶æ€§åˆ¤æ–­é˜ˆå€¼ï¼Œæ¶æ€§æ¦‚ç‡ >= threshold åˆ™åˆ¤ä¸ºæ¶æ€§ï¼ˆé»˜è®¤0.5ï¼‰
    """
    model.eval()
    
    all_results = {
        'indices': [],
        'types': [],
        'access_nos': [],
        'sop_uids': [],
        'bom_targets': [],
        'bom_preds': [],
        'bom_probs': []
    }
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Inference", ncols=80):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            images = batch['image'].to(device)
            
            # æ¨ç†
            outputs = model(images)
            bom_logits = outputs['bom']
            bom_probs = torch.softmax(bom_logits, dim=1)[:, 1]  # æ¶æ€§æ¦‚ç‡ï¼ˆç±»åˆ«1ï¼‰
            
            # ä½¿ç”¨é˜ˆå€¼åˆ¤æ–­ï¼ˆè€Œä¸æ˜¯argmaxï¼‰
            # æ¶æ€§æ¦‚ç‡ >= threshold â†’ åˆ¤ä¸ºæ¶æ€§(1)ï¼Œå¦åˆ™ä¸ºè‰¯æ€§(0)
            bom_preds = (bom_probs >= threshold).long()
            
            # æ”¶é›†ç»“æœ
            valid_mask = batch['bom_valid'] > 0
            if valid_mask.sum() > 0:
                all_results['indices'].extend(batch['idx'][valid_mask].cpu().numpy())
                all_results['types'].extend([batch['type'][i] for i in range(len(batch['type'])) if valid_mask[i]])
                all_results['access_nos'].extend([batch['access_no'][i] for i in range(len(batch['access_no'])) if valid_mask[i]])
                all_results['sop_uids'].extend([batch['sop_uid'][i] for i in range(len(batch['sop_uid'])) if valid_mask[i]])
                all_results['bom_targets'].extend(batch['bom'][valid_mask].cpu().numpy())
                all_results['bom_preds'].extend(bom_preds[valid_mask].cpu().numpy())
                all_results['bom_probs'].extend(bom_probs[valid_mask].cpu().numpy())
    
    return all_results

def calculate_metrics(targets, preds, probs):
    """è®¡ç®—BOMæŒ‡æ ‡"""
    metrics = {}
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
    if len(targets) == 0:
        return {'error': 'No valid samples'}
    
    targets = np.array(targets)
    preds = np.array(preds)
    probs = np.array(probs)
    
    # AUC
    if len(np.unique(targets)) >= 2:
        metrics['auc'] = roc_auc_score(targets, probs)
    else:
        metrics['auc'] = float('nan')
    
    # æ··æ·†çŸ©é˜µæŒ‡æ ‡
    tn, fp, fn, tp = confusion_matrix(targets, preds, labels=[0, 1]).ravel()
    
    metrics['accuracy'] = (tp + tn) / (tp + tn + fp + fn)
    metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0
    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0
    metrics['ppv'] = tp / (tp + fp) if (tp + fp) > 0 else 0
    metrics['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0
    
    metrics['tp'] = int(tp)
    metrics['tn'] = int(tn)
    metrics['fp'] = int(fp)
    metrics['fn'] = int(fn)
    metrics['total'] = len(targets)
    metrics['positive'] = int((targets == 1).sum())
    metrics['negative'] = int((targets == 0).sum())
    
    return metrics

# =============================================================================
# æŠ¥å‘Šç”Ÿæˆ
# =============================================================================

def save_predictions_to_csv(df_original, df_filtered, all_results, output_path, missing_samples):
    """
    å°†æ¨ç†ç»“æœå†™å›åˆ°åŸå§‹DataFrameå¹¶ä¿å­˜
    
    Args:
        df_original: åŸå§‹DataFrameï¼ˆåŒ…å«æ‰€æœ‰æ ·æœ¬ï¼‰
        df_filtered: è¿‡æ»¤åçš„DataFrameï¼ˆåªæœ‰æœ‰æ•ˆå›¾åƒçš„æ ·æœ¬ï¼‰
        all_results: æ¨ç†ç»“æœå­—å…¸
        output_path: è¾“å‡ºCSVè·¯å¾„
        missing_samples: ç¼ºå¤±å›¾åƒçš„æ ·æœ¬åˆ—è¡¨
    """
    # åˆ›å»ºä¸€ä¸ªå‰¯æœ¬
    df_output = df_original.copy()
    
    # åˆå§‹åŒ–æ–°åˆ—ï¼ˆæ‰€æœ‰æ ·æœ¬éƒ½è®¾ä¸ºNaNï¼‰
    df_output['bom_pred'] = np.nan
    df_output['bom_confidence'] = np.nan
    df_output['prediction_status'] = 'missing_image'  # é»˜è®¤çŠ¶æ€ï¼šç¼ºå¤±å›¾åƒ
    
    # å°†æ¨ç†ç»“æœå¡«å……åˆ°å¯¹åº”çš„æ ·æœ¬ä¸­
    # all_resultsä¸­çš„æ•°æ®å¯¹åº”df_filteredä¸­çš„æ ·æœ¬
    for idx in range(len(df_filtered)):
        # è·å–è¿‡æ»¤åDataFrameä¸­çš„access_noå’Œsop_uid
        row = df_filtered.iloc[idx]
        access_no = row['access_no']
        sop_uid = row['sop_uid']
        
        # åœ¨åŸå§‹DataFrameä¸­æ‰¾åˆ°å¯¹åº”çš„è¡Œ
        mask = (df_output['access_no'] == access_no) & (df_output['sop_uid'] == sop_uid)
        
        if mask.any():
            # è·å–é¢„æµ‹ç»“æœ
            pred = all_results['bom_preds'][idx]  # 0 æˆ– 1
            prob = all_results['bom_probs'][idx]  # æ¶æ€§çš„æ¦‚ç‡
            
            # å¡«å……é¢„æµ‹ç»“æœ
            df_output.loc[mask, 'bom_pred'] = int(pred)
            df_output.loc[mask, 'bom_confidence'] = float(prob)
            df_output.loc[mask, 'prediction_status'] = 'predicted'
    
    # ç»Ÿè®¡ä¿¡æ¯
    n_predicted = (df_output['prediction_status'] == 'predicted').sum()
    n_missing = (df_output['prediction_status'] == 'missing_image').sum()
    
    # ä¿å­˜åˆ°CSV
    df_output.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print(f"\nğŸ“Š é¢„æµ‹ç»“æœå·²ä¿å­˜:")
    print(f"  è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"  æ€»æ ·æœ¬æ•°: {len(df_output)}")
    print(f"  å·²é¢„æµ‹: {n_predicted} ({n_predicted/len(df_output)*100:.2f}%)")
    print(f"  ç¼ºå¤±å›¾åƒ: {n_missing} ({n_missing/len(df_output)*100:.2f}%)")
    print(f"\n  æ–°å¢åˆ—è¯´æ˜:")
    print(f"    - bom_pred: æ¨¡å‹é¢„æµ‹ç»“æœ (0=è‰¯æ€§, 1=æ¶æ€§)")
    print(f"    - bom_confidence: é¢„æµ‹ç½®ä¿¡åº¦ (æ¶æ€§æ¦‚ç‡, 0-1ä¹‹é—´)")
    print(f"    - prediction_status: é¢„æµ‹çŠ¶æ€ ('predicted'=å·²é¢„æµ‹, 'missing_image'=å›¾åƒç¼ºå¤±)")

def plot_performance_comparison(all_results, output_path):
    """
    ç»˜åˆ¶ä¸åŒæ•°æ®é›†çš„æ€§èƒ½å¯¹æ¯”å›¾
    
    Args:
        all_results: åŒ…å«æ‰€æœ‰é¢„æµ‹ç»“æœçš„å­—å…¸
        output_path: å›¾ç‰‡ä¿å­˜è·¯å¾„
    """
    # è®¡ç®—æ•´ä½“æŒ‡æ ‡
    overall_metrics = calculate_metrics(
        all_results['bom_targets'],
        all_results['bom_preds'],
        all_results['bom_probs']
    )
    
    # æŒ‰typeåˆ†ç»„è®¡ç®—æŒ‡æ ‡
    unique_types = sorted(set(all_results['types']))
    type_metrics = {}
    
    for data_type in unique_types:
        type_mask = np.array([t == data_type for t in all_results['types']])
        type_targets = np.array(all_results['bom_targets'])[type_mask]
        type_preds = np.array(all_results['bom_preds'])[type_mask]
        type_probs = np.array(all_results['bom_probs'])[type_mask]
        
        if len(type_targets) > 0:
            type_metrics[data_type] = calculate_metrics(type_targets, type_preds, type_probs)
    
    # å‡†å¤‡ç»˜å›¾æ•°æ®
    datasets = ['Overall'] + unique_types
    metrics_to_plot = ['AUC', 'Accuracy', 'Sensitivity', 'Specificity', 'PPV', 'NPV']
    
    data_for_plot = {metric: [] for metric in metrics_to_plot}
    
    # æ•´ä½“æŒ‡æ ‡
    data_for_plot['AUC'].append(overall_metrics['auc'])
    data_for_plot['Accuracy'].append(overall_metrics['accuracy'])
    data_for_plot['Sensitivity'].append(overall_metrics['sensitivity'])
    data_for_plot['Specificity'].append(overall_metrics['specificity'])
    data_for_plot['PPV'].append(overall_metrics['ppv'])
    data_for_plot['NPV'].append(overall_metrics['npv'])
    
    # å„typeæŒ‡æ ‡
    for data_type in unique_types:
        metrics = type_metrics[data_type]
        data_for_plot['AUC'].append(metrics['auc'] if not np.isnan(metrics['auc']) else 0)
        data_for_plot['Accuracy'].append(metrics['accuracy'])
        data_for_plot['Sensitivity'].append(metrics['sensitivity'])
        data_for_plot['Specificity'].append(metrics['specificity'])
        data_for_plot['PPV'].append(metrics['ppv'])
        data_for_plot['NPV'].append(metrics['npv'])
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('ç‹¬ç«‹éªŒè¯é›†æ€§èƒ½å¯¹æ¯” - V75æ¨¡å‹', fontsize=16, fontweight='bold')
    
    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']
    
    # ä¸ºæ¯ä¸ªæŒ‡æ ‡ç»˜åˆ¶æŸ±çŠ¶å›¾
    for idx, metric in enumerate(metrics_to_plot):
        row = idx // 3
        col = idx % 3
        ax = axes[row, col]
        
        x_pos = np.arange(len(datasets))
        bars = ax.bar(x_pos, data_for_plot[metric], color=colors[:len(datasets)], alpha=0.8, edgecolor='black')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, value) in enumerate(zip(bars, data_for_plot[metric])):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{value:.3f}',
                   ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        ax.set_ylabel('Score', fontsize=12)
        ax.set_title(metric, fontsize=14, fontweight='bold')
        ax.set_xticks(x_pos)
        ax.set_xticklabels(datasets, rotation=15, ha='right')
        ax.set_ylim(0, 1.0)
        ax.grid(axis='y', alpha=0.3, linestyle='--')
        ax.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, linewidth=1)
        ax.axhline(y=0.6, color='orange', linestyle='--', alpha=0.5, linewidth=1)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {output_path}")

def generate_report(all_results, output_path):
    """ç”Ÿæˆè¯¦ç»†çš„éªŒè¯æŠ¥å‘Š"""
    
    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("ç‹¬ç«‹éªŒè¯é›†æ¨ç†æŠ¥å‘Š - V75æ¨¡å‹")
    report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("=" * 80)
    report_lines.append("")
    
    # æ•´ä½“æŒ‡æ ‡
    report_lines.append("ğŸ“Š æ•´ä½“éªŒè¯é›†æ€§èƒ½:")
    report_lines.append(f"  æ€»æ ·æœ¬æ•°: {len(all_results['bom_targets'])}")
    
    overall_metrics = calculate_metrics(
        all_results['bom_targets'],
        all_results['bom_preds'],
        all_results['bom_probs']
    )
    
    report_lines.append(f"  è‰¯æ€§æ ·æœ¬: {overall_metrics['negative']} ({overall_metrics['negative']/overall_metrics['total']*100:.1f}%)")
    report_lines.append(f"  æ¶æ€§æ ·æœ¬: {overall_metrics['positive']} ({overall_metrics['positive']/overall_metrics['total']*100:.1f}%)")
    report_lines.append("")
    report_lines.append("  æ€§èƒ½æŒ‡æ ‡:")
    report_lines.append(f"    AUC:         {overall_metrics['auc']:.4f}")
    report_lines.append(f"    Accuracy:    {overall_metrics['accuracy']:.4f}")
    report_lines.append(f"    Sensitivity: {overall_metrics['sensitivity']:.4f}")
    report_lines.append(f"    Specificity: {overall_metrics['specificity']:.4f}")
    report_lines.append(f"    PPV:         {overall_metrics['ppv']:.4f}")
    report_lines.append(f"    NPV:         {overall_metrics['npv']:.4f}")
    report_lines.append("")
    report_lines.append("  æ··æ·†çŸ©é˜µ:")
    report_lines.append(f"    TP (çœŸé˜³æ€§): {overall_metrics['tp']}")
    report_lines.append(f"    TN (çœŸé˜´æ€§): {overall_metrics['tn']}")
    report_lines.append(f"    FP (å‡é˜³æ€§): {overall_metrics['fp']}")
    report_lines.append(f"    FN (å‡é˜´æ€§): {overall_metrics['fn']}")
    report_lines.append("")
    
    # æŒ‰typeåˆ†ç»„ç»Ÿè®¡
    report_lines.append("=" * 80)
    report_lines.append("ğŸ“Š æŒ‰æ•°æ®æ¥æºåˆ†ç»„çš„æ€§èƒ½åˆ†æ:")
    report_lines.append("=" * 80)
    report_lines.append("")
    
    # è·å–æ‰€æœ‰type
    unique_types = sorted(set(all_results['types']))
    
    for data_type in unique_types:
        # ç­›é€‰è¯¥typeçš„æ•°æ®
        type_mask = np.array([t == data_type for t in all_results['types']])
        type_targets = np.array(all_results['bom_targets'])[type_mask]
        type_preds = np.array(all_results['bom_preds'])[type_mask]
        type_probs = np.array(all_results['bom_probs'])[type_mask]
        
        report_lines.append(f"ğŸ” Type: {data_type}")
        report_lines.append(f"  æ ·æœ¬æ•°: {len(type_targets)}")
        
        if len(type_targets) > 0:
            type_metrics = calculate_metrics(type_targets, type_preds, type_probs)
            
            report_lines.append(f"  è‰¯æ€§æ ·æœ¬: {type_metrics['negative']} ({type_metrics['negative']/type_metrics['total']*100:.1f}%)")
            report_lines.append(f"  æ¶æ€§æ ·æœ¬: {type_metrics['positive']} ({type_metrics['positive']/type_metrics['total']*100:.1f}%)")
            report_lines.append("")
            report_lines.append("  æ€§èƒ½æŒ‡æ ‡:")
            
            auc_str = f"{type_metrics['auc']:.4f}" if not np.isnan(type_metrics['auc']) else "N/A (å•ç±»åˆ«)"
            report_lines.append(f"    AUC:         {auc_str}")
            report_lines.append(f"    Accuracy:    {type_metrics['accuracy']:.4f}")
            report_lines.append(f"    Sensitivity: {type_metrics['sensitivity']:.4f}")
            report_lines.append(f"    Specificity: {type_metrics['specificity']:.4f}")
            report_lines.append(f"    PPV:         {type_metrics['ppv']:.4f}")
            report_lines.append(f"    NPV:         {type_metrics['npv']:.4f}")
            report_lines.append("")
            report_lines.append("  æ··æ·†çŸ©é˜µ:")
            report_lines.append(f"    TP: {type_metrics['tp']}, TN: {type_metrics['tn']}, FP: {type_metrics['fp']}, FN: {type_metrics['fn']}")
        else:
            report_lines.append("  æ— æœ‰æ•ˆæ ·æœ¬")
        
        report_lines.append("")
        report_lines.append("-" * 80)
        report_lines.append("")
    
    # å†™å…¥æ–‡ä»¶
    report_content = "\n".join(report_lines)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
    print(report_content)
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")

# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    print("=" * 80)
    print("ç‹¬ç«‹éªŒè¯é›†æ¨ç† - V75æ¨¡å‹")
    print("=" * 80)
    
    device = get_device()
    print(f"ğŸ”¬ è®¾å¤‡: {device}")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {CONFIG['model_path']}")
    print(f"ğŸ“Š éªŒè¯æ•°æ®: {CONFIG['verify_data']}")
    print(f"ğŸ–¼ï¸ å›¾åƒæ ¹ç›®å½•: {CONFIG['verify_root']}")
    print()
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(CONFIG['model_path']):
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['model_path']}")
        return
    
    # åŠ è½½ç‰¹å¾æ˜ å°„
    with open(CONFIG['feature_mapping_file'], 'r', encoding='utf-8') as f:
        feature_mapping = json.load(f)
    print(f"âœ… åŠ è½½ç‰¹å¾æ˜ å°„: {list(feature_mapping.keys())}")
    
    # åŠ è½½éªŒè¯æ•°æ®
    df_verify_original = pd.read_csv(CONFIG['verify_data'])
    print(f"âœ… åŠ è½½éªŒè¯æ•°æ®: {len(df_verify_original)} æ ·æœ¬")
    print(f"   Typeåˆ†å¸ƒ: {df_verify_original['type'].value_counts().to_dict()}")
    print()
    
    # è¿‡æ»¤ç¼ºå¤±å›¾åƒ
    df_verify_filtered, missing_samples = filter_missing_images(df_verify_original, CONFIG['verify_root'])
    print()
    
    # å¦‚æœç¼ºå¤±å¤ªå¤šï¼Œç»™å‡ºè­¦å‘Š
    if len(missing_samples) > 0:
        missing_ratio = len(missing_samples) / len(df_verify_original) * 100
        if missing_ratio > 5:
            print(f"âš ï¸âš ï¸âš ï¸ è­¦å‘Š: ç¼ºå¤±å›¾åƒæ¯”ä¾‹è¾ƒé«˜ ({missing_ratio:.2f}%)ï¼Œå¯èƒ½å½±å“è¯„ä¼°å‡†ç¡®æ€§ï¼")
            print()
    
    # ä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®é›†è¿›è¡Œæ¨ç†
    df_verify = df_verify_filtered
    
    # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    verify_dataset = NoduleFeatureDataset(
        df_verify,
        CONFIG['verify_root'],
        feature_mapping,
        val_transform
    )
    
    verify_loader = DataLoader(
        verify_dataset,
        batch_size=CONFIG['batch_size'],
        shuffle=False,
        num_workers=CONFIG['num_workers']
    )
    
    print(f"âœ… åˆ›å»ºæ•°æ®é›†: {len(verify_dataset)} æ ·æœ¬")
    print()
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ”„ åŠ è½½æ¨¡å‹...")
    model = MultiTaskNoduleCNN(feature_mapping, dropout_rate=CONFIG['dropout_rate']).to(device)
    model.load_state_dict(torch.load(CONFIG['model_path'], map_location=device))
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print()
    
    # æ¨ç†
    print("ğŸš€ å¼€å§‹æ¨ç†...")
    print(f"âš™ï¸  è‰¯æ¶æ€§åˆ¤æ–­é˜ˆå€¼: {CONFIG['bom_threshold']} (æ¶æ€§æ¦‚ç‡ >= é˜ˆå€¼ â†’ åˆ¤ä¸ºæ¶æ€§)")
    all_results = inference_and_collect(model, verify_loader, device, threshold=CONFIG['bom_threshold'])
    print(f"âœ… æ¨ç†å®Œæˆ: æ”¶é›† {len(all_results['bom_targets'])} ä¸ªæœ‰æ•ˆæ ·æœ¬")
    print()
    
    # ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“ ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
    generate_report(all_results, CONFIG['output_report'])
    
    # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾
    print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾...")
    plot_path = CONFIG['output_report'].replace('.txt', '_comparison.png')
    plot_performance_comparison(all_results, plot_path)
    
    # ä¿å­˜é¢„æµ‹ç»“æœåˆ°CSV
    print("\nğŸ’¾ ä¿å­˜é¢„æµ‹ç»“æœåˆ°CSV...")
    save_predictions_to_csv(
        df_verify_original, 
        df_verify_filtered, 
        all_results, 
        CONFIG['output_csv'],
        missing_samples
    )

if __name__ == "__main__":
    main()
