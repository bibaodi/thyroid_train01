#!/usr/bin/env python3
"""
å°†å›¾åƒæ•°æ®é›†é¢„å¤„ç†æˆLMDBæ ¼å¼ï¼Œå¤§å¹…æå‡I/Oæ€§èƒ½
"""
import os
import sys
import json
import lmdb
import pickle
import pandas as pd
from PIL import Image
from tqdm import tqdm
import io
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def apply_filters(df, config):
    """åº”ç”¨ä¸è®­ç»ƒç›¸åŒçš„è¿‡æ»¤è§„åˆ™"""
    from train_nodule_feature_cnn_model_v75 import apply_all_filters
    return apply_all_filters(df, config, df_name="LMDB Data")

def create_lmdb_dataset(csv_path, image_root, output_path, map_size_gb=50, apply_training_filters=False):
    """
    å°†å›¾åƒæ•°æ®é›†è½¬æ¢ä¸ºLMDBæ ¼å¼
    
    Args:
        csv_path: CSVæ•°æ®æ–‡ä»¶è·¯å¾„
        image_root: å›¾åƒæ ¹ç›®å½•
        output_path: LMDBè¾“å‡ºè·¯å¾„
        map_size_gb: LMDBæœ€å¤§å¤§å°(GB)
        apply_training_filters: æ˜¯å¦åº”ç”¨è®­ç»ƒæ—¶çš„è¿‡æ»¤è§„åˆ™
    """
    print(f"ğŸ“Š åŠ è½½CSV: {csv_path}")
    df = pd.read_csv(csv_path, low_memory=False)
    print(f"   å…± {len(df)} æ¡è®°å½•")
    
    # åº”ç”¨è®­ç»ƒæ—¶çš„è¿‡æ»¤è§„åˆ™
    if apply_training_filters:
        print(f"\nğŸ” åº”ç”¨è®­ç»ƒè¿‡æ»¤è§„åˆ™...")
        from train_nodule_feature_cnn_model_v75 import CONFIG
        df = apply_filters(df, CONFIG)
        print(f"   è¿‡æ»¤å: {len(df)} æ¡è®°å½•")
    
    # åˆ›å»ºLMDB
    map_size = map_size_gb * 1024 * 1024 * 1024  # GB to bytes
    env = lmdb.open(output_path, map_size=map_size)
    
    success_count = 0
    fail_count = 0
    
    print(f"\nğŸ”„ å¼€å§‹å¤„ç†å›¾åƒ...")
    with env.begin(write=True) as txn:
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing"):
            access_no = row['access_no']
            sop_uid = row['sop_uid']
            image_path = os.path.join(image_root, access_no, f"{sop_uid}.jpg")
            
            try:
                # è¯»å–å›¾åƒä¸ºå­—èŠ‚
                with open(image_path, 'rb') as f:
                    image_bytes = f.read()
                
                # å‡†å¤‡æ•°æ®
                data = {
                    'image': image_bytes,
                    'access_no': access_no,
                    'sop_uid': sop_uid,
                    'bom': row.get('bom'),
                    'ti_rads': row.get('ti_rads'),
                    'composition': row.get('composition'),
                    'echo': row.get('echo'),
                    'foci': row.get('foci'),
                    'margin': row.get('margin'),
                    'shape': row.get('shape'),
                }
                
                # å­˜å‚¨
                key = f"{idx}".encode()
                value = pickle.dumps(data)
                txn.put(key, value)
                success_count += 1
                
            except FileNotFoundError:
                fail_count += 1
            except Exception as e:
                fail_count += 1
                if fail_count <= 5:
                    print(f"   Error at {idx}: {e}")
        
        # å­˜å‚¨å…ƒæ•°æ®
        meta = {
            'num_samples': success_count,
            'csv_path': csv_path,
            'image_root': image_root,
        }
        txn.put(b'__meta__', pickle.dumps(meta))
    
    env.close()
    
    print(f"\nâœ… LMDBåˆ›å»ºå®Œæˆ!")
    print(f"   æˆåŠŸ: {success_count}")
    print(f"   å¤±è´¥: {fail_count}")
    print(f"   è¾“å‡º: {output_path}")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    total_size = 0
    for f in os.listdir(output_path):
        total_size += os.path.getsize(os.path.join(output_path, f))
    print(f"   å¤§å°: {total_size / 1024 / 1024 / 1024:.2f} GB")

def main():
    parser = argparse.ArgumentParser(description='åˆ›å»ºLMDBæ•°æ®é›†')
    parser.add_argument('--csv', default='data/dataset_table/train/all_matched_sops_ds_v3_with_tr13_0926_with_OOF_suspect.csv')
    parser.add_argument('--image_root', default='data/dataset_images/2nodule_images')
    parser.add_argument('--output', default='data/dataset_lmdb/train_lmdb')
    parser.add_argument('--map_size', type=int, default=50, help='LMDB max size in GB')
    parser.add_argument('--filter', action='store_true', help='Apply training filters')
    args = parser.parse_args()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    
    create_lmdb_dataset(args.csv, args.image_root, args.output, args.map_size, args.filter)

if __name__ == '__main__':
    main()

